# Logical check for equality/similarity of different factors (title, author, DOI)
if (doi_exist) {
if ((title_identical & !doi_identical) |
doi_identical |
(common_authors & title_similar & !doi_identical)) {
duplicate[j] <- 1
}
} else {
if (title_identical |
(common_authors & title_similar)) {
duplicate[j] <- 1
}
}
}, error = function(err) { # Belongs to tryCatch
cat("Error at i =", i, "j =", j, ":", conditionMessage(err), "\n")
})
}
}
print(sum(duplicate))
# Insert duplicate column directly after the existing columns
data <- cbind(data, Duplicate = duplicate)
return(data)
}
data <- data[1:200, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
getAnywhere(dupliHunter())
#' data <- data.frame(
#'   PMID = 1:4,
#'   Title = c("Meta Analysis of Studies", "Meta-Analysis of Studies", "A Comprehensive Review", "Comprehensive Review"),
#'   Authors = c("John Doe", "J. Doe", "Jane Smith", "J. Smith"),
#'   DOI = c("10.1000/xyz123", "10.1000/xyz124", "10.1000/xyz125", "10.1000/xyz126"),
#'   stringsAsFactors = FALSE
#' )
#' duplicates <- dupliHunter(data, "Title", "Authors", "DOI")
#' print(duplicates)
#' @export
dupliHunter <- function(data, title_col, author_col, doi_col, threshold = 0.03) {
# Preprocess author names to remove diacritical marks and filter
preprocess_authors <- function(authors) {
words <- unlist(strsplit(authors, ' '))
words <- ifelse(length(words) <= 1, words, words[nchar(words) > 1])
has_diacritics <- function(x) any(grepl("[^a-zA-Z]", iconv(x, to = "ASCII//TRANSLIT")))
remove_diacritics <- function(x) iconv(x, to = "ASCII//TRANSLIT")
if (has_diacritics(words)) words <- remove_diacritics(words)
filtered_words <- words[!grepl("^[A-Z]+$", words)]
if (length(filtered_words) == 0) {
return("xxxx")
} else {
return(ifelse(length(filtered_words) > 2, filtered_words[1:2], filtered_words))
}
}
# Preprocess titles to escape special characters
preprocess_titles <- function(title) {
gsub("([][{}()+*^$|\\\\?.])", "\\\\\\1", title) # Filter special characters
}
preprocessed_authors <- lapply(data[[author_col]], preprocess_authors)
preprocessed_titles <- sapply(data[[title_col]], preprocess_titles)
# Function to check if two rows are duplicates
is_duplicate <- function(i, j, data, title_col, author_col, doi_col, threshold, preprocessed_titles, preprocessed_authors) {
title_i <- preprocessed_titles[i]
title_j <- preprocessed_titles[j]
title_identical <- grepl(title_i, title_j)
doi_exist <- data[[doi_col]][i] != "" && data[[doi_col]][j] != ""
doi_identical <- if (doi_exist) grepl(data[[doi_col]][i], data[[doi_col]][j]) | grepl(data[[doi_col]][j], data[[doi_col]][i]) else FALSE
similarity_score <- stringdist::stringdistmatrix(data[[title_col]][i], data[[title_col]][j], method = "jaccard")
title_similar <- (similarity_score <= threshold) & !title_identical
authors_i <- preprocessed_authors[[i]]
authors_j <- preprocessed_authors[[j]]
common_authors <- any(grepl(authors_i, authors_j) | grepl(authors_j, authors_i))
if (doi_exist) {
return((title_identical & !doi_identical) | doi_identical | (common_authors & title_similar & !doi_identical))
} else {
return(title_identical | (common_authors & title_similar))
}
}
# Use parallel processing to check for duplicates
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
# Ensure the packages are loaded in the cluster
clusterEvalQ(cl, {
library(stringdist)
})
# Export necessary variables and functions
clusterExport(cl, varlist = c("data", "title_col", "author_col", "doi_col", "threshold",
"preprocessed_titles", "preprocessed_authors", "is_duplicate",
"preprocess_authors", "grepl", "gsub"), envir = environment())
# Parallel computation using parLapply to ensure we get a list of lists
duplicate_list <- parLapply(cl, 1:(nrow(data) - 1), function(i) {
sapply((i + 1):nrow(data), function(j) {
is_duplicate(i, j, data, title_col, author_col, doi_col, threshold, preprocessed_titles, preprocessed_authors)
})
})
stopCluster(cl)
# Flatten the duplicate list and sum duplicates for each row
duplicate <- rep(0, nrow(data))  # Initialize with zeros
for (i in 1:(nrow(data) - 1)) {
duplicate[(i + 1):nrow(data)] <- duplicate[(i + 1):nrow(data)] + unlist(duplicate_list[[i]])
}
# Insert duplicate column directly after the existing columns
data <- cbind(data, Duplicate = duplicate)
return(data)
}
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
file.path <- "~/Nextcloud/project-fab-forschung/Projekte/Mawendo_II/Literaturrecherche/S83_K/Suche_DK/Trefferlisten/"
file.name <- "Trefferliste_allDatabases.csv"
if (grepl("csv", file.name)) {
data <- read.csv(paste0(file.path, file.name))
} else if (grepl("xlsx", file.name)) {
data <- readxl::read_excel(paste0(file.path, file.name))
}
data <- data[1:400, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
library(usethis)
library(devtools)
# devtools::install_github("koda86/DupliCheck")
library(DupliCheck)
file.path <- "~/Nextcloud/project-fab-forschung/Projekte/Mawendo_II/Literaturrecherche/S83_K/Suche_DK/Trefferlisten/"
file.name <- "Trefferliste_allDatabases.csv"
if (grepl("csv", file.name)) {
data <- read.csv(paste0(file.path, file.name))
} else if (grepl("xlsx", file.name)) {
data <- readxl::read_excel(paste0(file.path, file.name))
}
data <- data[1:400, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
#' data <- data.frame(
#'   PMID = 1:4,
#'   Title = c("Meta Analysis of Studies", "Meta-Analysis of Studies", "A Comprehensive Review", "Comprehensive Review"),
#'   Authors = c("John Doe", "J. Doe", "Jane Smith", "J. Smith"),
#'   DOI = c("10.1000/xyz123", "10.1000/xyz124", "10.1000/xyz125", "10.1000/xyz126"),
#'   stringsAsFactors = FALSE
#' )
#' duplicates <- dupliHunter(data, "Title", "Authors", "DOI")
#' print(duplicates)
#' @export
dupliHunter <- function(data, title_col, author_col, doi_col, threshold = 0.03) {
# Preprocess author names to remove diacritical marks and filter
preprocess_authors <- function(authors) {
words <- unlist(strsplit(authors, ' '))
words <- ifelse(length(words) <= 1, words, words[nchar(words) > 1])
has_diacritics <- function(x) any(grepl("[^a-zA-Z]", iconv(x, to = "ASCII//TRANSLIT")))
remove_diacritics <- function(x) iconv(x, to = "ASCII//TRANSLIT")
if (has_diacritics(words)) words <- remove_diacritics(words)
filtered_words <- words[!grepl("^[A-Z]+$", words)]
if (length(filtered_words) == 0) {
return("xxxx")
} else {
return(ifelse(length(filtered_words) > 2, filtered_words[1:2], filtered_words))
}
}
# Preprocess titles to escape special characters
preprocess_titles <- function(title) {
gsub("([][{}()+*^$|\\\\?.])", "\\\\\\1", title) # Filter special characters
}
preprocessed_authors <- lapply(data[[author_col]], preprocess_authors)
preprocessed_titles <- sapply(data[[title_col]], preprocess_titles)
# Function to check if two rows are duplicates
is_duplicate <- function(i, j, data, title_col, author_col, doi_col, threshold, preprocessed_titles, preprocessed_authors) {
title_i <- preprocessed_titles[i]
title_j <- preprocessed_titles[j]
title_identical <- grepl(title_i, title_j)
doi_exist <- data[[doi_col]][i] != "" && data[[doi_col]][j] != ""
doi_identical <- if (doi_exist) grepl(data[[doi_col]][i], data[[doi_col]][j]) | grepl(data[[doi_col]][j], data[[doi_col]][i]) else FALSE
similarity_score <- stringdist::stringdistmatrix(data[[title_col]][i], data[[title_col]][j], method = "jaccard")
title_similar <- (similarity_score <= threshold) & !title_identical
authors_i <- preprocessed_authors[[i]]
authors_j <- preprocessed_authors[[j]]
common_authors <- any(grepl(authors_i, authors_j) | grepl(authors_j, authors_i))
if (doi_exist) {
return((title_identical & !doi_identical) | doi_identical | (common_authors & title_similar & !doi_identical))
} else {
return(title_identical | (common_authors & title_similar))
}
}
# Use parallel processing to check for duplicates
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
# Ensure the packages are loaded in the cluster
clusterEvalQ(cl, {
library(stringdist)
})
# Export necessary variables and functions
clusterExport(cl, varlist = c("data", "title_col", "author_col", "doi_col", "threshold",
"preprocessed_titles", "preprocessed_authors", "is_duplicate",
"preprocess_authors", "grepl", "gsub"), envir = environment())
# Parallel computation using parLapply to ensure we get a list of lists
duplicate_list <- parLapply(cl, 1:(nrow(data) - 1), function(i) {
sapply((i + 1):nrow(data), function(j) {
is_duplicate(i, j, data, title_col, author_col, doi_col, threshold, preprocessed_titles, preprocessed_authors)
})
})
stopCluster(cl)
# Flatten the duplicate list and sum duplicates for each row
duplicate <- rep(0, nrow(data))  # Initialize with zeros
for (i in 1:(nrow(data) - 1)) {
duplicate[(i + 1):nrow(data)] <- duplicate[(i + 1):nrow(data)] + unlist(duplicate_list[[i]])
}
# Insert duplicate column directly after the existing columns
data <- cbind(data, Duplicate = duplicate)
return(data)
}
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
library(usethis)
library(devtools)
devtools::install_github("koda86/DupliCheck")
library(DupliCheck)
file.path <- "~/Nextcloud/project-fab-forschung/Projekte/Mawendo_II/Literaturrecherche/S83_K/Suche_DK/Trefferlisten/"
file.name <- "Trefferliste_allDatabases.csv"
if (grepl("csv", file.name)) {
data <- read.csv(paste0(file.path, file.name))
} else if (grepl("xlsx", file.name)) {
data <- readxl::read_excel(paste0(file.path, file.name))
}
data <- data[1:400, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
library(usethis)
library(devtools)
# devtools::install_github("koda86/DupliCheck")
library(DupliCheck)
file.path <- "~/Nextcloud/project-fab-forschung/Projekte/Mawendo_II/Literaturrecherche/S83_K/Suche_DK/Trefferlisten/"
file.name <- "Trefferliste_allDatabases.csv"
if (grepl("csv", file.name)) {
data <- read.csv(paste0(file.path, file.name))
} else if (grepl("xlsx", file.name)) {
data <- readxl::read_excel(paste0(file.path, file.name))
}
data <- data[1:400, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
devtools::install_github("koda86/DupliCheck")
library(DupliCheck)
file.path <- "~/Nextcloud/project-fab-forschung/Projekte/Mawendo_II/Literaturrecherche/S83_K/Suche_DK/Trefferlisten/"
file.name <- "Trefferliste_allDatabases.csv"
if (grepl("csv", file.name)) {
data <- read.csv(paste0(file.path, file.name))
} else if (grepl("xlsx", file.name)) {
data <- readxl::read_excel(paste0(file.path, file.name))
}
data <- data[1:400, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
devtools::install_github("koda86/DupliCheck")
library(DupliCheck)
file.path <- "~/Nextcloud/project-fab-forschung/Projekte/Mawendo_II/Literaturrecherche/S83_K/Suche_DK/Trefferlisten/"
file.name <- "Trefferliste_allDatabases.csv"
if (grepl("csv", file.name)) {
data <- read.csv(paste0(file.path, file.name))
} else if (grepl("xlsx", file.name)) {
data <- readxl::read_excel(paste0(file.path, file.name))
}
data <- data[1:400, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
#' data <- data.frame(
#'   PMID = 1:4,
#'   Title = c("Meta Analysis of Studies", "Meta-Analysis of Studies", "A Comprehensive Review", "Comprehensive Review"),
#'   Authors = c("John Doe", "J. Doe", "Jane Smith", "J. Smith"),
#'   DOI = c("10.1000/xyz123", "10.1000/xyz124", "10.1000/xyz125", "10.1000/xyz126"),
#'   stringsAsFactors = FALSE
#' )
#' duplicates <- dupliHunter(data, "Title", "Authors", "DOI")
#' print(duplicates)
#' @export
dupliHunter <- function(data, title_col, author_col, doi_col, threshold = 0.03) {
# Preprocess author names to remove diacritical marks and filter
preprocess_authors <- function(authors) {
words <- unlist(strsplit(authors, ' '))
words <- ifelse(length(words) <= 1, words, words[nchar(words) > 1])
has_diacritics <- function(x) any(grepl("[^a-zA-Z]", iconv(x, to = "ASCII//TRANSLIT")))
remove_diacritics <- function(x) iconv(x, to = "ASCII//TRANSLIT")
if (has_diacritics(words)) words <- remove_diacritics(words)
filtered_words <- words[!grepl("^[A-Z]+$", words)]
if (length(filtered_words) == 0) {
return("xxxx")
} else {
return(ifelse(length(filtered_words) > 2, filtered_words[1:2], filtered_words))
}
}
# Preprocess titles to escape special characters
preprocess_titles <- function(title) {
gsub("([][{}()+*^$|\\\\?.])", "\\\\\\1", title) # Filter special characters
}
preprocessed_authors <- lapply(data[[author_col]], preprocess_authors)
preprocessed_titles <- sapply(data[[title_col]], preprocess_titles)
# Function to check if two rows are duplicates
is_duplicate <- function(i, j, data, title_col, author_col, doi_col, threshold, preprocessed_titles, preprocessed_authors) {
title_i <- preprocessed_titles[i]
title_j <- preprocessed_titles[j]
title_identical <- grepl(title_i, title_j)
doi_exist <- data[[doi_col]][i] != "" && data[[doi_col]][j] != ""
doi_identical <- if (doi_exist) grepl(data[[doi_col]][i], data[[doi_col]][j]) | grepl(data[[doi_col]][j], data[[doi_col]][i]) else FALSE
similarity_score <- stringdist::stringdistmatrix(data[[title_col]][i], data[[title_col]][j], method = "jaccard")
title_similar <- (similarity_score <= threshold) & !title_identical
authors_i <- preprocessed_authors[[i]]
authors_j <- preprocessed_authors[[j]]
common_authors <- any(grepl(authors_i, authors_j) | grepl(authors_j, authors_i))
if (doi_exist) {
return((title_identical & !doi_identical) | doi_identical | (common_authors & title_similar & !doi_identical))
} else {
return(title_identical | (common_authors & title_similar))
}
}
# Use parallel processing to check for duplicates
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
# Ensure the packages are loaded in the cluster
clusterEvalQ(cl, {
library(stringdist)
})
# Export necessary variables and functions
clusterExport(cl, varlist = c("data", "title_col", "author_col", "doi_col", "threshold",
"preprocessed_titles", "preprocessed_authors", "is_duplicate",
"preprocess_authors", "grepl", "gsub"), envir = environment())
# Parallel computation using parLapply to ensure we get a list of lists
duplicate_list <- parLapply(cl, 1:(nrow(data) - 1), function(i) {
sapply((i + 1):nrow(data), function(j) {
is_duplicate(i, j, data, title_col, author_col, doi_col, threshold, preprocessed_titles, preprocessed_authors)
})
})
stopCluster(cl)
# Flatten the duplicate list and sum duplicates for each row
duplicate <- rep(0, nrow(data))  # Initialize with zeros
for (i in 1:(nrow(data) - 1)) {
duplicate[(i + 1):nrow(data)] <- duplicate[(i + 1):nrow(data)] + unlist(duplicate_list[[i]])
}
# Insert duplicate column directly after the existing columns
data <- cbind(data, Duplicate = duplicate)
return(data)
}
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
sessionInfo()
getwd()
devtools::install()
devtools::check()
devtools::document()
rm(list=c("dupliHunter"))
devtools::load_all()
devtools::document()
devtools::check()
devtools::load_all()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
git add .
devtools::install_github("koda86/DupliCheck")
library(DupliCheck)
file.path <- "~/Nextcloud/project-fab-forschung/Projekte/Mawendo_II/Literaturrecherche/S83_K/Suche_DK/Trefferlisten/"
file.name <- "Trefferliste_allDatabases.csv"
if (grepl("csv", file.name)) {
data <- read.csv(paste0(file.path, file.name))
} else if (grepl("xlsx", file.name)) {
data <- readxl::read_excel(paste0(file.path, file.name))
}
data <- data[1:400, ]
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
View(data)
#' @return A data frame with an additional column 'Duplicate' indicating duplicates.
#' @examples
#' data <- data.frame(
#'   Title = c("Meta Analysis of Studies", "Meta-Analysis of Studies", "A Comprehensive Review", "Comprehensive Review"),
#'   Authors = c("John Doe", "J. Doe", "Jane Smith", "J. Smith"),
#'   DOI = c("10.1000/xyz123", "10.1000/xyz124", "10.1000/xyz125", "10.1000/xyz126"),
#'   stringsAsFactors = FALSE
#' )
#' duplicates <- dupliHunter(data, "Title", "Authors", "DOI")
#' print(duplicates)
dupliHunter <- function(data, title_col, author_col, doi_col, threshold = 0.03) {
duplicate <- integer(nrow(data))
for (i in 1:(nrow(data) - 1)) {
if (duplicate[i] == 1) next # Skip rows already marked as duplicates
title_identical <- FALSE
authors_identical <- FALSE
doi_identical <- FALSE
# Filter author names
words <- unlist(strsplit(data[[author_col]][i], ' '))
words <- ifelse(length(words) <= 1, words, words[nchar(words) > 1])
# Remove diacritical marks if present
has_diacritics <- function(x) {
any(grepl("[^a-zA-Z]", iconv(x, to = "ASCII//TRANSLIT")))
}
remove_diacritics <- function(x) {iconv(x, to = "ASCII//TRANSLIT")}
if (has_diacritics(words)) {words <- remove_diacritics(words)}
filtered_words <- words[!grepl("^[A-Z]+$", words)]
if (length(filtered_words) == 0) {
authors_i <- "xxxx"
} else {
authors_i <- ifelse(length(filtered_words) > 2, filtered_words[1:2], filtered_words)
}
for (j in (i + 1):nrow(data)) {
tryCatch({
if (duplicate[j] == 1) next # Skip rows already marked as duplicates
# Check for identical pairs
title_i <- data[[title_col]][i]
title_j <- data[[title_col]][j]
title_i_escaped <- gsub("([][{}()+*^$|\\\\?.])", "\\\\\\1", title_i) # Filter special characters
title_j_escaped <- gsub("([][{}()+*^$|\\\\?.])", "\\\\\\1", title_j)
title_identical <- grepl(title_i_escaped, title_j_escaped)
if (data[[doi_col]][i] != "" && data[[doi_col]][j] != "") {
doi_exist <- TRUE
doi_identical <- grepl(data[[doi_col]][i], data[[doi_col]][j]) | grepl(data[[doi_col]][j], data[[doi_col]][i])
} else {
doi_identical <- FALSE
}
# Check for "sufficient" similarity between title pairs
similarity_score <- stringdist::stringdistmatrix(data[[title_col]][i], data[[title_col]][j], method = "jaccard")
title_similar <- (similarity_score <= threshold) & !title_identical
# Filter author names
words <- unlist(strsplit(data[[author_col]][j], ' '))
words <- ifelse(length(words) <= 1, words, words[nchar(words) > 1])
# Remove diacritical marks if present
if (has_diacritics(words)) {words <- remove_diacritics(words)}
filtered_words <- words[!grepl("^[A-Z]+$", words)]
if (length(filtered_words) == 0 | is.na(filtered_words)) {
authors_j <- "yyyy"
} else {
authors_j <- ifelse(length(filtered_words) > 2, filtered_words[1:2], filtered_words)
}
common_authors <- any(grepl(authors_i, authors_j) | grepl(authors_j, authors_i))
# Logical check for equality/similarity of different factors (title, author, DOI)
if (doi_exist) {
if ((title_identical & !doi_identical) |
doi_identical |
(common_authors & title_similar & !doi_identical)) {
duplicate[j] <- 1
}
} else {
if (title_identical |
(common_authors & title_similar)) {
duplicate[j] <- 1
}
}
}, error = function(err) { # Belongs to tryCatch
cat("Error at i =", i, "j =", j, ":", conditionMessage(err), "\n")
})
}
}
print(sum(duplicate))
# Insert duplicate column directly after the existing columns
data <- cbind(data, Duplicate = duplicate)
return(data)
}
system.time(
duplicates <- dupliHunter(data,
title_col = "Title",
author_col = "Authors",
doi_col = "DOI",
threshold = 0.03)
)
